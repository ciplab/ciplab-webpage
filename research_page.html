<!DOCTYPE html>
<html lang="ko">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Research</title>
  <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700;900&display=swap" rel="stylesheet">
  <style>
    * { box-sizing: border-box; }
    body {
      font-family: 'Montserrat', sans-serif;
      margin: 0;
      padding: 0;
      overflow-x: hidden;
      background-color: #fff;
      padding-bottom: 100px; /* Ï∂îÍ∞ÄÎêú ÌïòÎã® Ïó¨Î∞± */
    }
    .header {
      background-image: url('https://lh6.googleusercontent.com/4vyttQZKKqWizrNZxayrfiVEojwOZmxUdnqUw9WMWfX3kWTnhd6i5IG4vMTn4Neat0GoW-6CPjtPLwL0p2cA-Arj79aPftpnUjwWdk_MfIbrD_CyT6SbGUUH9OySIBMRDg=w1280');
      /* https://lh5.googleusercontent.com/ErFPRLmpuLIUkm6Eq5a-sNruIY7Vg7_PsmwQKjt4aDHbNUcu-_F26iCM9KqGyWxehaK-76oAbI5RW0igssWvvuDhbiK-1fNtcC5al8wSIhcnZEGigm9siRVg6DSB9E6N6w=w1280 */
      height: 30vh;
      display: flex;
      justify-content: center;
      align-items: center;
      background-size: cover;
      background-position: center;
      position: relative;
    }
    .header::before {
      content: "";
      position: absolute;
      top: 0;
      left: 0;
      right: 0;
      bottom: 0;
      background: rgba(0, 0, 0, 0.2);
    }
    .header-text {
      color: white;
      font-size: 2.7em;
      font-weight: bold;
      position: relative;
      z-index: 1;
      padding: 0 10px;
    }
    #sections-container {
      display: flex;
      flex-direction: column;
      margin: 20px auto;
      width: 95%;
      max-width: 1200px;
    }
    .section {
      display: flex;
      padding: 15px 0;
      align-items: center;
    }
    .section:not(:last-child) {
      border-bottom: 1px solid #ddd;
      margin-bottom: 20px;
    }
    .image-container {
      width: 395px;
      height: 237px;
      margin-right: 20px;
      margin-bottom: 20px;
      background-color: #f0f0f0;
      display: flex;
      justify-content: center;
      align-items: center;
    }
    .image-container img {
      width: 100%;
      height: 100%;
      object-fit: cover;
    }
    .content-container {
      flex: 1;
    }
    .research-title {
      font-size: 18pt;
      margin: 0 0 10px 0;
      font-weight: 700;
      color: #1c4587;
    }
    .list-title {
      font-size: 12pt;
      margin: 10px 0 5px 0;
      font-weight: bold;
      color: #1c4587;
    }
    ul {
      margin: 0;
      padding-left: 20px;
    }
    ul li {
      margin-bottom: 5px; /* Í∞Å Ìï≠Î™© ÏÇ¨Ïù¥Ïóê 5px Í∞ÑÍ≤© Ï∂îÍ∞Ä */
      /* ÎòêÎäî ÌïÑÏöî Ïãú, line-height: 1.5; ÏôÄ Í∞ôÏù¥ Ï°∞Ï†ï */
    }
    /* Î™®Î∞îÏùº Î∞è ÌÉúÎ∏îÎ¶ø ÌôòÍ≤Ω: Î†àÏù¥ÏïÑÏõÉ Î∞è Ìå®Îî© Ï°∞Ï†ï */
    @media (max-width: 768px) {
      .section {
        flex-direction: column;
        align-items: center;
      }
      .image-container {
        width: 100%;
        max-width: 300px;
        height: auto;
        margin-right: 0;
        margin-bottom: 15px;
      }
      .content-container {
        width: 100%;
        padding: 0 15px;
        text-align: left;
      }
      .content-container .research-title {
        text-align: center;
      }
    }
  </style>
</head>
<body>
  <div class="header">
    <div class="header-text">Research</div>
  </div>
  <div id="sections-container">
    <div class="section" id="section-generative">
      <div class="image-container" style="width: 257px; height: 257px; margin: 0px 79px 0px 79px;">
        <img src="https://github.com/ciplab/ciplab-webpage/blob/main/data/assets/Generative_AI.gif?raw=true" alt="Sample Image">
      </div>
      <div class="content-container">
        <div class="research-title">Generative AI</div>
        <div class="list-title">[Tasks]</div>
        <ul>
          <li>Understanding Diffusion models</li>
          <li>Conditional Video Generation (Portrait, Motion, etc.)</li>
          <li>Compositional Image Manipulation (Editing, Customization, etc.)</li>
          <li>Evaluation Metrics for Diffusion models</li>
        </ul>
        <div class="list-title">[Selected Papers]</div>
        <ul>
          <li>IF-MDM: Implicit Face Motion Diffusion Model for High-Fidelity Realtime Talking Head Generation, <span style="font-size: 12px; font-style: italic; color: #3d85c6;">under review</span></li>
          <li>Latent space Super-Resolution for Higher-Resolution Image Generation with Diffusion Models, <span style="font-size: 12px; font-style: italic; color: #3d85c6;">CVPR 2025</span></li>
          <li>ORIDa: Object-centric Real-world Image Composition Dataset, <span style="font-size: 12px; font-style: italic; color: #3d85c6;">CVPR 2025</span></li>
        </ul>
      </div>
    </div>
    <div class="section" id="section-video">
      <div class="image-container">
        <img src="https://github.com/ciplab/ciplab-webpage/blob/main/data/assets/Video_Understanding.gif?raw=true" alt="Sample Image">
      </div>
      <div class="content-container">
        <div class="research-title">Video Understanding</div>
        <div class="list-title">[Tasks]</div>
        <ul>
          <li>Streaming Video Understanding in Efficient Architecture (SSM, Efficient Transformer) </li>
          <li>Video-Language Understanding</li>
          <li>Vision for Robotics</li>
        </ul>
        <div class="list-title">[Selected Papers]</div>
        <ul>
          <li>Multi-Granular Spatio-Temporal Token Merging for Training-Free Acceleration of Video LLMs, <span style="font-size: 12px; font-style: italic; color: #3d85c6;"> under review</span></li>
          <li>Imitating Human Videos via Cross-Embodiment Skill Representations, <span style="font-size: 12px; font-style: italic; color: #3d85c6;"> under review</span></li>
          <li>Omni-RGPT: Unifying Image and Video Region-level Understanding via Token Marks, <span style="font-size: 12px; font-style: italic; color: #3d85c6;">CVPR 2025</span>
            <a href="https://arxiv.org/abs/2501.08326" target="_blank" rel="noopener noreferrer" style="text-decoration: none;">üîó</a></li>
          <li>ActionSwitch: Class-agnostic Detection of Simultaneous Actions in Streaming Videos, <span style="font-size: 12px; font-style: italic; color: #3d85c6;">ECCV 2024</span>
                          <a href="https://musicaloffering.github.io/ActionSwitch-release/" target="_blank" rel="noopener noreferrer" style="text-decoration: none;">üîó</a></li>
          <li>Video Object Segmentation using Space-Time Memory Networks, <span style="font-size: 12px; font-style: italic; color: #3d85c6;">ICCV 2019 (Oral)</span>
                          <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Oh_Video_Object_Segmentation_Using_Space-Time_Memory_Networks_ICCV_2019_paper.pdf" target="_blank" rel="noopener noreferrer" style="text-decoration: none;">üîó</a></li>
        </ul>
      </div>
    </div>
    <div class="section" id="section-3d">
      <div class="image-container">
        <img src="https://github.com/ciplab/ciplab-webpage/blob/main/data/assets/3D_Vision.png?raw=true" alt="Sample Image">
      </div>
      <div class="content-container">
        <div class="research-title">3D vision</div>
        <div class="list-title">[Tasks]</div>
        <ul>
          <li>Novel-View Synthesis (NeRF, Gaussian Splatting, etc.)</li>
          <li>3D Representation Learning</li>
          <li>Animatable 3D Model Reconstruction</li>
          <li>Point Cloud Understanding</li>
        </ul>
        <div class="list-title">[Selected Papers]</div>
        <ul>
          <li>Hierarchically Structured Neural Bones for Reconstructing Animatable Objects from Casual Videos, <span style="font-size: 12px; font-style: italic; color: #3d85c6;">ECCV 2024</span>
                          <a href="https://sites.google.com/yonsei.ac.kr/subinjeon/projects/hsnb" target="_blank" rel="noopener noreferrer" style="text-decoration: none;">üîó</a></li>
          <li>Learning to Enhance Aperture Phasor Field for Non-Line-of-Sight Imaging, <span style="font-size: 12px; font-style: italic; color: #3d85c6;">ECCV 2024</span>
                          <a href="https://join16.github.io/leap-page/" target="_blank" rel="noopener noreferrer" style="text-decoration: none;">üîó</a></li>
          <li>EPINET: A Fully-Convolutional Neural Network Using Epipolar Geometry for Depth from Light Field Images, <span style="font-size: 12px; font-style: italic; color: #3d85c6;">CVPR 2018</span>
                          <a href="https://arxiv.org/abs/1804.02379" target="_blank" rel="noopener noreferrer" style="text-decoration: none;">üîó</a></li>
        </ul>
      </div>
    </div>
    <div class="section" id="section-lowlevel">
      <div class="image-container">
        <img src="https://github.com/ciplab/ciplab-webpage/blob/main/data/assets/Low-level_Vision.jpg?raw=true" alt="Sample Image">
      </div>
      <div class="content-container">
        <div class="research-title">Low-level Vision & Computational Photography</div>
        <div class="list-title">[Tasks]</div>
        <ul>
          <li>Low-level vision : In-camera Pipeline, Auto White Balance, Super Resolution, etc.</li>
          <li>Image Quality Assessment</li>
          <li>Image / Video Manipulation</li>
        </ul>
        <div class="list-title">[Selected Papers]</div>
        <ul>
          <li>ATTIQA: Generalizable Image Quality Feature Extractor using Attribute-aware Pretraining, <span style="font-size: 12px; font-style: italic; color: #3d85c6;">ACCV 2025</span>
                          <a href="https://arxiv.org/abs/2406.01020" target="_blank" rel="noopener noreferrer" style="text-decoration: none;">üîó</a></li>
          <li>Accelerating Large Image Super-Resolution Networks with Pixel-Level Classification, <span style="font-size: 12px; font-style: italic; color: #3d85c6;">ECCV 2024</span>
                          <a href="https://www.dykim.me/projects/aid" target="_blank" rel="noopener noreferrer" style="text-decoration: none;">üîó</a></li>
          <li>Attentive Illumination Decomposition Model for Multi-Illuminant White Balance, <span style="font-size: 12px; font-style: italic; color: #3d85c6;">CVPR 2024</span>
                          <a href="https://3587jjh.github.io/PCSR/" target="_blank" rel="noopener noreferrer" style="text-decoration: none;">üîó</a></li>
          <li>Dense Interspecies Face Embedding, <span style="font-size: 12px; font-style: italic; color: #3d85c6;">NeurIPS 2022</span>
                          <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/d71a4a6c796cacd9b8a298589943cdf3-Paper-Conference.pdf" target="_blank" rel="noopener noreferrer" style="text-decoration: none;">üîó</a></li>
        </ul>
      </div>
    </div>
  </div>
  <script>
    function shuffleSections() {
      const container = document.getElementById('sections-container');
      const sections = Array.from(container.children);
      sections.sort(() => Math.random() - 0.5);
      sections.forEach(section => container.appendChild(section));
    }
    window.onload = shuffleSections;
  </script>
</body>
</html>